{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4f15bbf5",
      "metadata": {},
      "source": [
        "# Modelling\n",
        "\n",
        "In this notebook, we build a predictive model. For assessing the quality of our model, we select the true positive rate. This choosing is warranted by the following substantial arguments."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30b8d20c",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "The following cell enumerates all modules requisite for the modelling tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "762eeb67",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from ipynb_utils.cfg import CFG\n",
        "from ipynb_utils import dump_df, load_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e59f29ec",
      "metadata": {},
      "source": [
        "Let us reload the resulting data from the preceding notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7b1d8c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = load_data(\"1--df_retrieved.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70fa6f88",
      "metadata": {},
      "source": [
        "Firstly, we shall split the set of data points into feature and training as well as training and test parts, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1383431d",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_cols_blacklist = [\n",
        "    \"target\",\n",
        "]\n",
        "X_cols = [col for col in df.columns if col not in X_cols_blacklist]\n",
        "\n",
        "y_cols_whitelist = [\n",
        "    \"target\",\n",
        "]\n",
        "y_cols = [col for col in df.columns if col in y_cols_whitelist]\n",
        "\n",
        "X = df[X_cols].values\n",
        "y = df[y_cols].values.ravel()\n",
        "\n",
        "X_0, X_1, y_0, y_1 = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=CFG[\"RSEED\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65bd2b32",
      "metadata": {},
      "source": [
        "The subsequent cell block creates a model fitted to the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4c1024f",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = LogisticRegression()\n",
        "model.fit(X_0, y_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27709da4",
      "metadata": {},
      "source": [
        "As the final step, we shall evaluate the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86967a1d",
      "metadata": {},
      "outputs": [],
      "source": [
        "z_1 = model.predict(X_1)\n",
        "\n",
        "accuracy = accuracy_score(y_1, z_1)\n",
        "precision = precision_score(y_1, z_1)\n",
        "recall = recall_score(y_1, z_1)\n",
        "f1 = f1_score(y_1, z_1)\n",
        "cm = confusion_matrix(y_1, z_1)\n",
        "\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(f\"  Accuracy:  {accuracy:.2f}\")\n",
        "print(f\"  Precision: {precision:.2f}\")\n",
        "print(f\"  Recall:    {recall:.2f}\")\n",
        "print(f\"  f1 Score:  {f1:.2f}\")\n",
        "print(\"\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
